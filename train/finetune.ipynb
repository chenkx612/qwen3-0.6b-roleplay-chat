{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 微信聊天角色扮演 - Qwen3-0.6B 微调\n",
        "\n",
        "使用LoRA微调Qwen3-0.6B，让模型学习模仿聊天记录中对方的说话风格。\n",
        "\n",
        "## 环境要求\n",
        "- Google Colab (GPU)\n",
        "- 免费版T4 GPU即可运行"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 安装依赖\n",
        "!pip install -q transformers>=4.40.0 peft>=0.10.0 trl>=0.8.0 datasets accelerate bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    TrainingArguments,\n",
        "    BitsAndBytesConfig\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM\n",
        "from datasets import Dataset\n",
        "import json\n",
        "\n",
        "# 检查GPU\n",
        "print(f\"GPU可用: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU型号: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"显存: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. 上传训练数据\n",
        "\n",
        "将预处理好的 `train_data.json` 上传到Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "\n",
        "# 上传训练数据\n",
        "uploaded = files.upload()\n",
        "data_file = list(uploaded.keys())[0]\n",
        "print(f\"已上传: {data_file}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 加载训练数据\n",
        "with open(data_file, 'r', encoding='utf-8') as f:\n",
        "    train_data = json.load(f)\n",
        "\n",
        "print(f\"训练样本数量: {len(train_data)}\")\n",
        "print(f\"\\n样本示例:\")\n",
        "print(json.dumps(train_data[0], ensure_ascii=False, indent=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. 加载模型和Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "MODEL_ID = \"Qwen/Qwen3-0.6B\"\n",
        "\n",
        "# 加载Tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "# 设置pad_token\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "print(f\"词表大小: {len(tokenizer)}\")\n",
        "print(f\"特殊token: pad={tokenizer.pad_token}, eos={tokenizer.eos_token}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4-bit量化配置（节省显存）\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True\n",
        ")\n",
        "\n",
        "# 加载模型\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "# 准备模型进行k-bit训练\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "print(f\"模型加载完成\")\n",
        "print(f\"模型参数量: {model.num_parameters() / 1e6:.1f}M\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. 配置LoRA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# LoRA配置\n",
        "lora_config = LoraConfig(\n",
        "    r=16,                    # LoRA秩\n",
        "    lora_alpha=32,           # 缩放系数\n",
        "    target_modules=[\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",  # 注意力层\n",
        "        \"gate_proj\", \"up_proj\", \"down_proj\"       # FFN层\n",
        "    ],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "# 应用LoRA\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "# 打印可训练参数\n",
        "model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. 准备数据集"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def format_conversation(example):\n",
        "    \"\"\"\n",
        "    将对话格式化为Qwen3的chat模板\n",
        "    \"\"\"\n",
        "    messages = example[\"conversations\"]\n",
        "    \n",
        "    # 使用tokenizer的chat模板\n",
        "    text = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=False\n",
        "    )\n",
        "    \n",
        "    return {\"text\": text}\n",
        "\n",
        "# 创建Dataset\n",
        "dataset = Dataset.from_list(train_data)\n",
        "dataset = dataset.map(format_conversation)\n",
        "\n",
        "print(f\"数据集大小: {len(dataset)}\")\n",
        "print(f\"\\n格式化后的样本:\")\n",
        "print(dataset[0][\"text\"][:500])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. 训练配置"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 训练参数\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./output\",\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=4,\n",
        "    gradient_accumulation_steps=4,\n",
        "    learning_rate=2e-4,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    warmup_ratio=0.1,\n",
        "    fp16=True,\n",
        "    logging_steps=10,\n",
        "    save_strategy=\"epoch\",\n",
        "    save_total_limit=2,\n",
        "    optim=\"paged_adamw_8bit\",\n",
        "    report_to=\"none\",\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "print(\"训练配置:\")\n",
        "print(f\"  Epochs: {training_args.num_train_epochs}\")\n",
        "print(f\"  Batch size: {training_args.per_device_train_batch_size}\")\n",
        "print(f\"  Gradient accumulation: {training_args.gradient_accumulation_steps}\")\n",
        "print(f\"  Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
        "print(f\"  Learning rate: {training_args.learning_rate}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 创建Trainer\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset,\n",
        "    processing_class=tokenizer,\n",
        "    max_seq_length=512,\n",
        "    dataset_text_field=\"text\",\n",
        "    packing=False\n",
        ")\n",
        "\n",
        "print(\"Trainer已创建，准备开始训练\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. 开始训练"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 开始训练\n",
        "print(\"开始训练...\")\n",
        "trainer.train()\n",
        "print(\"训练完成！\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. 保存模型"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 保存LoRA适配器\n",
        "LORA_OUTPUT_DIR = \"./lora_adapter\"\n",
        "model.save_pretrained(LORA_OUTPUT_DIR)\n",
        "tokenizer.save_pretrained(LORA_OUTPUT_DIR)\n",
        "\n",
        "print(f\"LoRA适配器已保存至: {LORA_OUTPUT_DIR}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# （可选）合并为完整模型\n",
        "MERGE_OUTPUT_DIR = \"./merged_model\"\n",
        "\n",
        "# 重新加载基座模型（不量化）\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "# 加载LoRA权重并合并\n",
        "from peft import PeftModel\n",
        "merged_model = PeftModel.from_pretrained(base_model, LORA_OUTPUT_DIR)\n",
        "merged_model = merged_model.merge_and_unload()\n",
        "\n",
        "# 保存合并后的模型\n",
        "merged_model.save_pretrained(MERGE_OUTPUT_DIR)\n",
        "tokenizer.save_pretrained(MERGE_OUTPUT_DIR)\n",
        "\n",
        "print(f\"合并后的模型已保存至: {MERGE_OUTPUT_DIR}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. 测试模型"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def chat(model, tokenizer, user_input, history=[]):\n",
        "    \"\"\"\n",
        "    单轮对话测试\n",
        "    \"\"\"\n",
        "    messages = history + [{\"role\": \"user\", \"content\": user_input}]\n",
        "    \n",
        "    prompt = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True\n",
        "    )\n",
        "    \n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=128,\n",
        "            do_sample=True,\n",
        "            temperature=0.7,\n",
        "            top_p=0.9,\n",
        "            repetition_penalty=1.1,\n",
        "            pad_token_id=tokenizer.pad_token_id\n",
        "        )\n",
        "    \n",
        "    generated = outputs[0][inputs[\"input_ids\"].shape[1]:]\n",
        "    response = tokenizer.decode(generated, skip_special_tokens=True)\n",
        "    \n",
        "    return response.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 测试对话\n",
        "test_inputs = [\n",
        "    \"在干嘛\",\n",
        "    \"今天累不累\",\n",
        "    \"晚上吃什么\",\n",
        "    \"一起看电影吧\"\n",
        "]\n",
        "\n",
        "print(\"=== 模型测试 ===\")\n",
        "for user_input in test_inputs:\n",
        "    response = chat(merged_model, tokenizer, user_input)\n",
        "    print(f\"用户: {user_input}\")\n",
        "    print(f\"模型: {response}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. 下载模型\n",
        "\n",
        "将训练好的模型下载到本地"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 打包LoRA适配器\n",
        "!zip -r lora_adapter.zip ./lora_adapter\n",
        "\n",
        "# 下载\n",
        "from google.colab import files\n",
        "files.download('lora_adapter.zip')\n",
        "\n",
        "print(\"LoRA适配器已下载，解压后放到本地项目目录使用\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# （可选）打包合并后的完整模型\n",
        "# 注意：完整模型文件较大，下载可能需要较长时间\n",
        "!zip -r merged_model.zip ./merged_model\n",
        "\n",
        "from google.colab import files\n",
        "files.download('merged_model.zip')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 本地使用方法\n",
        "\n",
        "```bash\n",
        "# 方法1: 使用LoRA适配器\n",
        "python inference/chat.py --model Qwen/Qwen3-0.6B --lora ./lora_adapter\n",
        "\n",
        "# 方法2: 使用合并后的模型\n",
        "python inference/chat.py --model ./merged_model\n",
        "```"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
